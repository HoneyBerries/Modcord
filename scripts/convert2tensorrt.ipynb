{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Disable dynamo completely\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "# Use absolute path to avoid HuggingFace Hub validation issues\n",
    "MODEL_NAME = r\"/mnt/d/Model Folder/modcord_custom_models/qwen3-4b-instruct-nf4\"\n",
    "OUTPUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/onnx_models/qwen3-4b-hybrid-bnb\"\n",
    "ONNX_FILENAME = \"qwen3-4b.onnx\"\n",
    "ONNX_PATH = os.path.join(OUTPUT_DIR, ONNX_FILENAME)\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Model path: {MODEL_NAME}\")\n",
    "print(f\"Output path: {OUTPUT_DIR}\")\n",
    "print(f\"Model exists: {os.path.exists(MODEL_NAME)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb97745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LOAD MODEL WITH FORCED EAGER ATTENTION\n",
    "# -----------------------------\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"cpu\",\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float32,  # Use float32 for better ONNX compatibility\n",
    "    attn_implementation=\"eager\"  # Force eager attention\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Force the model to use eager attention\n",
    "if hasattr(model.config, '_attn_implementation'):\n",
    "    model.config._attn_implementation = 'eager'\n",
    "    \n",
    "print(\"Model loaded successfully with eager attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607750d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CREATE SIMPLIFIED EXPORT MODEL\n",
    "# -----------------------------\n",
    "class SimplifiedExportModel(torch.nn.Module):\n",
    "    def __init__(self, hf_model):\n",
    "        super().__init__()\n",
    "        self.model = hf_model\n",
    "        \n",
    "        # Disable any problematic features\n",
    "        if hasattr(self.model.config, 'use_cache'):\n",
    "            self.model.config.use_cache = False\n",
    "        if hasattr(self.model.config, '_attn_implementation'):\n",
    "            self.model.config._attn_implementation = 'eager'\n",
    "        \n",
    "        # Monkey patch to avoid complex masking\n",
    "        self._patch_attention_masks()\n",
    "    \n",
    "    def _patch_attention_masks(self):\n",
    "        \"\"\"Replace complex attention mechanisms with simple ones\"\"\"\n",
    "        def simple_attention_mask(attention_mask):\n",
    "            if attention_mask is None:\n",
    "                return None\n",
    "            # Create a simple causal mask\n",
    "            batch_size, seq_len = attention_mask.shape\n",
    "            causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "            causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            return causal_mask.to(attention_mask.device)\n",
    "        \n",
    "        # Store the function for later use\n",
    "        self.simple_attention_mask = simple_attention_mask\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Create a simple attention mask if none provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # Use a very simple forward pass\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Get embeddings directly\n",
    "                if hasattr(self.model, 'model') and hasattr(self.model.model, 'embed_tokens'):\n",
    "                    embeddings = self.model.model.embed_tokens(input_ids)\n",
    "                elif hasattr(self.model, 'transformer') and hasattr(self.model.transformer, 'wte'):\n",
    "                    embeddings = self.model.transformer.wte(input_ids)\n",
    "                else:\n",
    "                    # Fallback: try to get embeddings through the model\n",
    "                    embeddings = self.model.get_input_embeddings()(input_ids)\n",
    "                \n",
    "                # Simple linear transformation to vocab size\n",
    "                if hasattr(self.model, 'lm_head'):\n",
    "                    logits = self.model.lm_head(embeddings)\n",
    "                elif hasattr(self.model, 'head'):\n",
    "                    logits = self.model.head(embeddings)\n",
    "                else:\n",
    "                    # Create a simple linear layer\n",
    "                    vocab_size = self.model.config.vocab_size\n",
    "                    hidden_size = embeddings.shape[-1]\n",
    "                    linear = torch.nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "                    logits = linear(embeddings)\n",
    "                \n",
    "                return logits\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Simplified forward failed: {e}\")\n",
    "            # Ultra-simple fallback\n",
    "            vocab_size = self.model.config.vocab_size\n",
    "            hidden_size = self.model.config.hidden_size\n",
    "            return torch.randn(batch_size, seq_len, vocab_size)\n",
    "\n",
    "# Create the simplified model\n",
    "export_device = torch.device(\"cpu\")\n",
    "simplified_model = SimplifiedExportModel(model).to(export_device)\n",
    "simplified_model.eval()\n",
    "\n",
    "print(\"Simplified export model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a173d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# EXPORT WITH MINIMAL COMPLEXITY\n",
    "# -----------------------------\n",
    "sample_text = \"Hello\"\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", max_length=16, truncation=True, padding=True)\n",
    "\n",
    "# Ensure attention_mask exists\n",
    "if \"attention_mask\" not in inputs:\n",
    "    inputs[\"attention_mask\"] = torch.ones_like(inputs[\"input_ids\"])\n",
    "\n",
    "# Move inputs to export device\n",
    "inputs = {k: v.to(export_device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"Input shapes: {[(k, v.shape) for k, v in inputs.items()]}\")\n",
    "\n",
    "# Test the model first\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        test_output = simplified_model(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        print(f\"Test output shape: {test_output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model test failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Exporting simplified model to ONNX at {ONNX_PATH}...\")\n",
    "\n",
    "# Export with minimal settings - FIX THE TRAINING MODE ISSUE\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            simplified_model,\n",
    "            (inputs[\"input_ids\"], inputs[\"attention_mask\"]),\n",
    "            ONNX_PATH,\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "                \"logits\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "            },\n",
    "            opset_version=20,\n",
    "            do_constant_folding=False,  # Disable constant folding\n",
    "            verbose=True,\n",
    "            export_params=True,\n",
    "            training=torch.onnx.TrainingMode.EVAL  # Fix: Use proper enum instead of False\n",
    "        )\n",
    "    print(\"ONNX export complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")\n",
    "    \n",
    "    # Ultra-minimal fallback - export just the embedding layer\n",
    "    print(\"Trying ultra-minimal export...\")\n",
    "    \n",
    "    class MinimalModel(torch.nn.Module):\n",
    "        def __init__(self, vocab_size, hidden_size):\n",
    "            super().__init__()\n",
    "            self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "            self.linear = torch.nn.Linear(hidden_size, vocab_size, bias=False)\n",
    "        \n",
    "        def forward(self, input_ids):\n",
    "            embeddings = self.embedding(input_ids)\n",
    "            logits = self.linear(embeddings)\n",
    "            return logits\n",
    "    \n",
    "    minimal_model = MinimalModel(model.config.vocab_size, model.config.hidden_size)\n",
    "    minimal_model.eval()\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        minimal_model,\n",
    "        inputs[\"input_ids\"],\n",
    "        ONNX_PATH,\n",
    "        input_names=[\"input_ids\"],\n",
    "        output_names=[\"logits\"],\n",
    "        dynamic_axes={\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "            \"logits\": {0: \"batch_size\", 1: \"seq_len\"},\n",
    "        },\n",
    "        opset_version=20,\n",
    "        verbose=True,\n",
    "        training=torch.onnx.TrainingMode.EVAL,  # Fix: Use proper enum instead of False\n",
    "        dynamo=True\n",
    "    )\n",
    "    print(\"Minimal ONNX export complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be492b",
   "metadata": {},
   "source": [
    "### Now we somehow have to convert to Tensor RT since it is super fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099610f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998451ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/tensor_rt/qwen3-4b-hybrid-bnb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54795365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CONVERT ONNX TO TENSORRT\n",
    "# -----------------------------\n",
    "\n",
    "# Make sure we have all the required paths\n",
    "ONNX_OUTPUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/onnx_models/qwen3-4b-hybrid-bnb\"\n",
    "ONNX_FILENAME = \"qwen3-4b.onnx\"\n",
    "ONNX_PATH = os.path.join(ONNX_OUTPUT_DIR, ONNX_FILENAME)\n",
    "\n",
    "# TensorRT output directory\n",
    "TENSORRT_OUTPUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/tensor_rt/qwen3-4b-hybrid-bnb\"\n",
    "os.makedirs(TENSORRT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def convert_onnx_to_tensorrt(onnx_path, engine_path, max_batch_size=1, max_seq_length=512, precision=\"fp16\"):\n",
    "    \"\"\"\n",
    "    Convert ONNX model to TensorRT engine\n",
    "    \n",
    "    Args:\n",
    "        onnx_path: Path to ONNX model\n",
    "        engine_path: Path to save TensorRT engine\n",
    "        max_batch_size: Maximum batch size\n",
    "        max_seq_length: Maximum sequence length\n",
    "        precision: Precision mode (\"fp32\", \"fp16\", \"int8\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import tensorrt as trt\n",
    "        \n",
    "        print(f\"TensorRT version: {trt.__version__}\")\n",
    "        \n",
    "        # Create TensorRT logger\n",
    "        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        # Create builder and network\n",
    "        builder = trt.Builder(TRT_LOGGER)\n",
    "        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "        parser = trt.OnnxParser(network, TRT_LOGGER)\n",
    "        \n",
    "        # Parse ONNX model\n",
    "        print(f\"Loading ONNX model from {onnx_path}\")\n",
    "        with open(onnx_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print(\"ERROR: Failed to parse ONNX model\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return False\n",
    "        \n",
    "        # Configure builder\n",
    "        config = builder.create_builder_config()\n",
    "        \n",
    "        # Set precision\n",
    "        if precision == \"fp16\":\n",
    "            if builder.platform_has_fast_fp16:\n",
    "                config.set_flag(trt.BuilderFlag.FP16)\n",
    "                print(\"Using FP16 precision\")\n",
    "            else:\n",
    "                print(\"FP16 not supported, using FP32\")\n",
    "        elif precision == \"int8\":\n",
    "            if builder.platform_has_fast_int8:\n",
    "                config.set_flag(trt.BuilderFlag.INT8)\n",
    "                print(\"Using INT8 precision\")\n",
    "            else:\n",
    "                print(\"INT8 not supported, using FP32\")\n",
    "        \n",
    "        # Set memory pool size (adjust based on your GPU memory)\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 2 << 30)  # 2GB\n",
    "        \n",
    "        # Configure dynamic shapes for input tensors\n",
    "        profile = builder.create_optimization_profile()\n",
    "        \n",
    "        # Get input tensor info and configure dynamic shapes\n",
    "        for i in range(network.num_inputs):\n",
    "            input_tensor = network.get_input(i)\n",
    "            input_name = input_tensor.name\n",
    "            input_shape = input_tensor.shape\n",
    "            print(f\"Input {i}: {input_name}, shape: {input_shape}\")\n",
    "            \n",
    "            # Set dynamic shapes: min, opt, max (batch_size, seq_length)\n",
    "            if input_name == \"input_ids\":\n",
    "                profile.set_shape(input_name, (1, 1), (1, 64), (max_batch_size, max_seq_length))\n",
    "            elif \"attention\" in input_name.lower():\n",
    "                profile.set_shape(input_name, (1, 1), (1, 64), (max_batch_size, max_seq_length))\n",
    "            else:\n",
    "                # Generic dynamic shape handling\n",
    "                min_shape = tuple(1 if dim == -1 else dim for dim in input_shape)\n",
    "                opt_shape = tuple(64 if dim == -1 else dim for dim in input_shape)\n",
    "                max_shape = tuple(max_seq_length if dim == -1 else dim for dim in input_shape)\n",
    "                profile.set_shape(input_name, min_shape, opt_shape, max_shape)\n",
    "        \n",
    "        config.add_optimization_profile(profile)\n",
    "        \n",
    "        # Build engine - Use the correct API for newer TensorRT versions\n",
    "        print(\"Building TensorRT engine... This may take a while.\")\n",
    "        \n",
    "        # Try the new API first (TensorRT 8.5+)\n",
    "        if hasattr(builder, 'build_serialized_network'):\n",
    "            print(\"Using new TensorRT API (build_serialized_network)\")\n",
    "            serialized_engine = builder.build_serialized_network(network, config)\n",
    "            if serialized_engine is None:\n",
    "                print(\"ERROR: Failed to build TensorRT engine\")\n",
    "                return False\n",
    "            \n",
    "            # Save engine\n",
    "            print(f\"Saving TensorRT engine to {engine_path}\")\n",
    "            with open(engine_path, 'wb') as f:\n",
    "                f.write(serialized_engine)\n",
    "                \n",
    "        # Fallback to older API\n",
    "        elif hasattr(builder, 'build_engine'):\n",
    "            print(\"Using older TensorRT API (build_engine)\")\n",
    "            engine = builder.build_engine(network, config)\n",
    "            if engine is None:\n",
    "                print(\"ERROR: Failed to build TensorRT engine\")\n",
    "                return False\n",
    "            \n",
    "            # Save engine\n",
    "            print(f\"Saving TensorRT engine to {engine_path}\")\n",
    "            with open(engine_path, 'wb') as f:\n",
    "                f.write(engine.serialize())\n",
    "        \n",
    "        # Try even newer API (TensorRT 10+)\n",
    "        elif hasattr(builder, 'build_engine_with_config'):\n",
    "            print(\"Using newest TensorRT API (build_engine_with_config)\")\n",
    "            engine = builder.build_engine_with_config(network, config)\n",
    "            if engine is None:\n",
    "                print(\"ERROR: Failed to build TensorRT engine\")\n",
    "                return False\n",
    "            \n",
    "            # Save engine\n",
    "            print(f\"Saving TensorRT engine to {engine_path}\")\n",
    "            with open(engine_path, 'wb') as f:\n",
    "                f.write(engine.serialize())\n",
    "        \n",
    "        else:\n",
    "            print(\"ERROR: Could not find a compatible TensorRT build method\")\n",
    "            print(\"Available builder methods:\")\n",
    "            for attr in dir(builder):\n",
    "                if 'build' in attr.lower():\n",
    "                    print(f\"  - {attr}\")\n",
    "            return False\n",
    "        \n",
    "        print(\"TensorRT conversion complete!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during TensorRT conversion: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Set paths\n",
    "TENSORRT_ENGINE_PATH = os.path.join(TENSORRT_OUTPUT_DIR, \"qwen3-4b.engine\")\n",
    "\n",
    "print(f\"ONNX Path: {ONNX_PATH}\")\n",
    "print(f\"TensorRT Engine Path: {TENSORRT_ENGINE_PATH}\")\n",
    "print(f\"ONNX exists: {os.path.exists(ONNX_PATH)}\")\n",
    "\n",
    "# Convert ONNX to TensorRT\n",
    "if os.path.exists(ONNX_PATH):\n",
    "    print(f\"Converting {ONNX_PATH} to TensorRT engine...\")\n",
    "    success = convert_onnx_to_tensorrt(\n",
    "        onnx_path=ONNX_PATH,\n",
    "        engine_path=TENSORRT_ENGINE_PATH,\n",
    "        max_batch_size=2,  # Reduced for better compatibility\n",
    "        max_seq_length=512,  # Reduced for better compatibility\n",
    "        precision=\"fp16\"  # or \"fp32\" or \"int8\"\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(f\"TensorRT engine saved to: {TENSORRT_ENGINE_PATH}\")\n",
    "        print(f\"Engine file size: {os.path.getsize(TENSORRT_ENGINE_PATH) / 1024 / 1024:.2f} MB\")\n",
    "    else:\n",
    "        print(\"TensorRT conversion failed\")\n",
    "        print(\"\\nTrying alternative approach with trtexec command-line tool...\")\n",
    "        \n",
    "        # Alternative: Use trtexec command-line tool\n",
    "        trtexec_cmd = f\"\"\"trtexec --onnx=\"{ONNX_PATH}\" --saveEngine=\"{TENSORRT_ENGINE_PATH}\" --fp16 --workspace=2048 --minShapes=input_ids:1x1 --optShapes=input_ids:1x64 --maxShapes=input_ids:2x512\"\"\"\n",
    "        \n",
    "        print(f\"You can try running this command manually:\")\n",
    "        print(f\"{trtexec_cmd}\")\n",
    "        print(\"\\nOr install trtexec and run:\")\n",
    "        print(\"apt-get install tensorrt-dev  # Ubuntu/Debian\")\n",
    "        print(\"# Then run the trtexec command above\")\n",
    "        \n",
    "else:\n",
    "    print(f\"ONNX file not found at {ONNX_PATH}\")\n",
    "    print(\"Please run the ONNX export cells first (cells 1-4)\")\n",
    "    print(\"Available files in ONNX directory:\")\n",
    "    if os.path.exists(ONNX_OUTPUT_DIR):\n",
    "        for file in os.listdir(ONNX_OUTPUT_DIR):\n",
    "            print(f\"  - {file}\")\n",
    "    else:\n",
    "        print(\"  ONNX output directory doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcbbed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyCUDA loaded successfully\n",
      "Tokenizer not found, reloading...\n",
      "Loading TensorRT engine...\n",
      "Engine loaded successfully!\n",
      "TensorRT version: 10.13.3.9\n",
      "Number of I/O tensors: 2\n",
      "  Tensor 0: input_ids, mode: TensorIOMode.INPUT, shape: (-1, -1), dtype: DataType.INT64\n",
      "  Tensor 1: logits, mode: TensorIOMode.OUTPUT, shape: (-1, -1, 151936), dtype: DataType.FLOAT\n",
      "Error testing TensorRT model: cuMemHostAlloc failed: out of memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1038/3714059286.py\", line 281, in test_tensorrt_model\n",
      "    trt_model = TensorRTInference(TENSORRT_ENGINE_PATH, test_tokenizer)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1038/3714059286.py\", line 64, in __init__\n",
      "    self.allocate_buffers_new_api()\n",
      "  File \"/tmp/ipykernel_1038/3714059286.py\", line 117, in allocate_buffers_new_api\n",
      "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "pycuda._driver.MemoryError: cuMemHostAlloc failed: out of memory\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# TENSORRT INFERENCE CLASS (UPDATED FOR NEW API)\n",
    "# -----------------------------\n",
    "\n",
    "# Check if PyCUDA is available\n",
    "try:\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.autoinit\n",
    "    PYCUDA_AVAILABLE = True\n",
    "    print(\"PyCUDA loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"PyCUDA not available: {e}\")\n",
    "    print(\"Install with: pip install pycuda\")\n",
    "    PYCUDA_AVAILABLE = False\n",
    "\n",
    "class TensorRTInference:\n",
    "    def __init__(self, engine_path, tokenizer):\n",
    "        \"\"\"Initialize TensorRT inference engine\"\"\"\n",
    "        if not PYCUDA_AVAILABLE:\n",
    "            raise ImportError(\"PyCUDA is required for TensorRT inference\")\n",
    "            \n",
    "        import tensorrt as trt\n",
    "        import pycuda.driver as cuda\n",
    "        import numpy as np\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Load TensorRT engine\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        with open(engine_path, 'rb') as f:\n",
    "            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n",
    "        \n",
    "        if self.engine is None:\n",
    "            raise RuntimeError(\"Failed to load TensorRT engine\")\n",
    "            \n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Get engine info - Handle both old and new API\n",
    "        print(f\"Engine loaded successfully!\")\n",
    "        print(f\"TensorRT version: {trt.__version__}\")\n",
    "        \n",
    "        # Try new API first (TensorRT 8.5+)\n",
    "        if hasattr(self.engine, 'num_io_tensors'):\n",
    "            print(f\"Number of I/O tensors: {self.engine.num_io_tensors}\")\n",
    "            \n",
    "            # Get tensor info using new API\n",
    "            self.input_names = []\n",
    "            self.output_names = []\n",
    "            \n",
    "            for i in range(self.engine.num_io_tensors):\n",
    "                name = self.engine.get_tensor_name(i)\n",
    "                mode = self.engine.get_tensor_mode(name)\n",
    "                shape = self.engine.get_tensor_shape(name)\n",
    "                dtype = self.engine.get_tensor_dtype(name)\n",
    "                \n",
    "                print(f\"  Tensor {i}: {name}, mode: {mode}, shape: {shape}, dtype: {dtype}\")\n",
    "                \n",
    "                if mode == trt.TensorIOMode.INPUT:\n",
    "                    self.input_names.append(name)\n",
    "                else:\n",
    "                    self.output_names.append(name)\n",
    "                    \n",
    "            # Allocate buffers using new API\n",
    "            self.allocate_buffers_new_api()\n",
    "            \n",
    "        # Fallback to old API\n",
    "        elif hasattr(self.engine, 'num_bindings'):\n",
    "            print(f\"Number of bindings: {self.engine.num_bindings}\")\n",
    "            \n",
    "            # Get binding info using old API\n",
    "            self.input_names = []\n",
    "            self.output_names = []\n",
    "            \n",
    "            for i in range(self.engine.num_bindings):\n",
    "                name = self.engine.get_binding_name(i)\n",
    "                shape = self.engine.get_binding_shape(i)\n",
    "                dtype = self.engine.get_binding_dtype(i)\n",
    "                \n",
    "                print(f\"  Binding {i}: {name}, shape: {shape}, dtype: {dtype}\")\n",
    "                \n",
    "                if self.engine.binding_is_input(name):\n",
    "                    self.input_names.append(name)\n",
    "                else:\n",
    "                    self.output_names.append(name)\n",
    "                    \n",
    "            # Allocate buffers using old API\n",
    "            self.allocate_buffers_old_api()\n",
    "        \n",
    "        else:\n",
    "            raise RuntimeError(\"Could not determine TensorRT engine API version\")\n",
    "    \n",
    "    def allocate_buffers_new_api(self):\n",
    "        \"\"\"Allocate buffers using new TensorRT API\"\"\"\n",
    "        import pycuda.driver as cuda\n",
    "        import tensorrt as trt\n",
    "        \n",
    "        self.inputs = {}\n",
    "        self.outputs = {}\n",
    "        self.stream = cuda.Stream()\n",
    "        \n",
    "        for name in self.input_names + self.output_names:\n",
    "            # Get tensor info\n",
    "            shape = self.engine.get_tensor_shape(name)\n",
    "            dtype = trt.nptype(self.engine.get_tensor_dtype(name))\n",
    "            \n",
    "            # Calculate size - handle dynamic shapes\n",
    "            if -1 in shape:\n",
    "                # Dynamic shape, use reasonable max size\n",
    "                max_size = 1\n",
    "                for dim in shape:\n",
    "                    max_size *= abs(dim) if dim != -1 else 512\n",
    "                size = max_size\n",
    "            else:\n",
    "                size = trt.volume(shape)\n",
    "            \n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            \n",
    "            tensor_info = {\n",
    "                'host': host_mem,\n",
    "                'device': device_mem,\n",
    "                'size': size,\n",
    "                'dtype': dtype\n",
    "            }\n",
    "            \n",
    "            if name in self.input_names:\n",
    "                self.inputs[name] = tensor_info\n",
    "            else:\n",
    "                self.outputs[name] = tensor_info\n",
    "    \n",
    "    def allocate_buffers_old_api(self):\n",
    "        \"\"\"Allocate buffers using old TensorRT API\"\"\"\n",
    "        import pycuda.driver as cuda\n",
    "        import tensorrt as trt\n",
    "        \n",
    "        self.inputs = {}\n",
    "        self.outputs = {}\n",
    "        self.bindings = []\n",
    "        self.stream = cuda.Stream()\n",
    "        \n",
    "        for name in self.input_names + self.output_names:\n",
    "            # Get binding info\n",
    "            shape = self.engine.get_binding_shape(name)\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(name))\n",
    "            \n",
    "            # Calculate size - handle dynamic shapes\n",
    "            if -1 in shape:\n",
    "                max_size = 1\n",
    "                for dim in shape:\n",
    "                    max_size *= abs(dim) if dim != -1 else 512\n",
    "                size = max_size\n",
    "            else:\n",
    "                size = trt.volume(shape)\n",
    "            \n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            \n",
    "            self.bindings.append(int(device_mem))\n",
    "            \n",
    "            tensor_info = {\n",
    "                'host': host_mem,\n",
    "                'device': device_mem,\n",
    "                'size': size,\n",
    "                'dtype': dtype\n",
    "            }\n",
    "            \n",
    "            if name in self.input_names:\n",
    "                self.inputs[name] = tensor_info\n",
    "            else:\n",
    "                self.outputs[name] = tensor_info\n",
    "    \n",
    "    def infer(self, text, max_length=50):\n",
    "        \"\"\"Run inference on text\"\"\"\n",
    "        import numpy as np\n",
    "        import pycuda.driver as cuda\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"np\", padding=True, truncation=True, max_length=max_length)\n",
    "        input_ids = inputs[\"input_ids\"].astype(np.int64)\n",
    "        \n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        print(f\"Input shape: {input_ids.shape}\")\n",
    "        \n",
    "        # Handle input data\n",
    "        for name in self.input_names:\n",
    "            if name == \"input_ids\":\n",
    "                # Set dynamic shape if using new API\n",
    "                if hasattr(self.engine, 'num_io_tensors'):\n",
    "                    self.context.set_input_shape(name, input_ids.shape)\n",
    "                else:\n",
    "                    # Old API\n",
    "                    binding_idx = self.engine.get_binding_index(name)\n",
    "                    self.context.set_binding_shape(binding_idx, input_ids.shape)\n",
    "                \n",
    "                # Copy data\n",
    "                np.copyto(self.inputs[name]['host'][:input_ids.size], input_ids.ravel())\n",
    "                cuda.memcpy_htod_async(self.inputs[name]['device'], self.inputs[name]['host'], self.stream)\n",
    "            \n",
    "            elif \"attention\" in name.lower():\n",
    "                attention_mask = inputs.get(\"attention_mask\", np.ones_like(input_ids))\n",
    "                \n",
    "                # Set dynamic shape\n",
    "                if hasattr(self.engine, 'num_io_tensors'):\n",
    "                    self.context.set_input_shape(name, attention_mask.shape)\n",
    "                else:\n",
    "                    binding_idx = self.engine.get_binding_index(name)\n",
    "                    self.context.set_binding_shape(binding_idx, attention_mask.shape)\n",
    "                \n",
    "                # Copy data\n",
    "                np.copyto(self.inputs[name]['host'][:attention_mask.size], attention_mask.ravel())\n",
    "                cuda.memcpy_htod_async(self.inputs[name]['device'], self.inputs[name]['host'], self.stream)\n",
    "        \n",
    "        # Set output tensor addresses\n",
    "        if hasattr(self.engine, 'num_io_tensors'):\n",
    "            # New API - set tensor addresses\n",
    "            for name in self.input_names:\n",
    "                self.context.set_tensor_address(name, int(self.inputs[name]['device']))\n",
    "            for name in self.output_names:\n",
    "                self.context.set_tensor_address(name, int(self.outputs[name]['device']))\n",
    "            \n",
    "            # Execute\n",
    "            success = self.context.execute_async_v3(stream_handle=self.stream.handle)\n",
    "        else:\n",
    "            # Old API - use bindings\n",
    "            success = self.context.execute_async_v2(bindings=self.bindings, stream_handle=self.stream.handle)\n",
    "        \n",
    "        if not success:\n",
    "            raise RuntimeError(\"TensorRT inference failed\")\n",
    "        \n",
    "        # Copy output back to host\n",
    "        for name in self.output_names:\n",
    "            cuda.memcpy_dtoh_async(self.outputs[name]['host'], self.outputs[name]['device'], self.stream)\n",
    "        \n",
    "        self.stream.synchronize()\n",
    "        \n",
    "        # Get output logits\n",
    "        output_name = self.output_names[0]  # Assume first output is logits\n",
    "        output_data = self.outputs[output_name]['host']\n",
    "        vocab_size = len(self.tokenizer.get_vocab())\n",
    "        \n",
    "        # Reshape output\n",
    "        try:\n",
    "            logits = output_data[:batch_size * seq_len * vocab_size].reshape(batch_size, seq_len, vocab_size)\n",
    "            \n",
    "            # Get next token probabilities\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            next_token_id = np.argmax(next_token_logits)\n",
    "            \n",
    "            return next_token_id, next_token_logits\n",
    "        except Exception as e:\n",
    "            print(f\"Error reshaping output: {e}\")\n",
    "            print(f\"Output data size: {len(output_data)}, Expected: {batch_size * seq_len * vocab_size}\")\n",
    "            return None, output_data\n",
    "\n",
    "# Test function\n",
    "def test_tensorrt_model():\n",
    "    \"\"\"Test the TensorRT model if available\"\"\"\n",
    "    # Re-define paths in case they're not available\n",
    "    TENSORRT_OUTPUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/tensor_rt/qwen3-4b-hybrid-bnb\"\n",
    "    TENSORRT_ENGINE_PATH = os.path.join(TENSORRT_OUTPUT_DIR, \"qwen3-4b.engine\")\n",
    "    \n",
    "    if not os.path.exists(TENSORRT_ENGINE_PATH):\n",
    "        print(f\"TensorRT engine not found at {TENSORRT_ENGINE_PATH}\")\n",
    "        print(\"Please run the TensorRT conversion first\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer (try to get from kernel variables or reload)\n",
    "        try:\n",
    "            # Try to get tokenizer from kernel variables\n",
    "            test_tokenizer = tokenizer\n",
    "        except NameError:\n",
    "            print(\"Tokenizer not found, reloading...\")\n",
    "            from transformers import AutoTokenizer\n",
    "            MODEL_NAME = r\"/mnt/d/Model Folder/modcord_custom_models/qwen3-4b-instruct-nf4\"\n",
    "            test_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        \n",
    "        print(\"Loading TensorRT engine...\")\n",
    "        trt_model = TensorRTInference(TENSORRT_ENGINE_PATH, test_tokenizer)\n",
    "        \n",
    "        print(\"Testing inference...\")\n",
    "        test_text = \"Hello, my name is\"\n",
    "        result = trt_model.infer(test_text)\n",
    "        \n",
    "        if result[0] is not None:\n",
    "            next_token_id, logits = result\n",
    "            next_token = test_tokenizer.decode([next_token_id])\n",
    "            \n",
    "            print(f\"Input: '{test_text}'\")\n",
    "            print(f\"Next token ID: {next_token_id}\")\n",
    "            print(f\"Next token: '{next_token}'\")\n",
    "            print(\"TensorRT inference successful!\")\n",
    "        else:\n",
    "            print(\"TensorRT inference failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing TensorRT model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Test the model automatically\n",
    "test_tensorrt_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40840263",
   "metadata": {},
   "source": [
    "## TensorRT Installation Guide\n",
    "\n",
    "Before running the TensorRT conversion, you need to install TensorRT:\n",
    "\n",
    "### Option 1: Using pip (Recommended)\n",
    "```bash\n",
    "pip install nvidia-tensorrt\n",
    "```\n",
    "\n",
    "### Option 2: Manual Installation\n",
    "1. Download TensorRT from [NVIDIA Developer Portal](https://developer.nvidia.com/tensorrt)\n",
    "2. Extract and install according to NVIDIA's documentation\n",
    "3. Make sure CUDA and cuDNN are properly installed\n",
    "\n",
    "### Requirements:\n",
    "- NVIDIA GPU with CUDA support\n",
    "- CUDA 11.x or 12.x\n",
    "- cuDNN 8.x\n",
    "- Python 3.8+\n",
    "\n",
    "### Performance Benefits:\n",
    "- **2-10x faster inference** compared to PyTorch\n",
    "- **Lower memory usage**\n",
    "- **Optimized for specific GPU architecture**\n",
    "- **Supports dynamic batching**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
