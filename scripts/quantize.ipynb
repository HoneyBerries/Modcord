{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d541b1e5",
   "metadata": {},
   "source": [
    "# Quantize a loaded model to 4-bit\n",
    "This notebook demonstrates loading a causal LM and converting its weights to a 4-bit quant format (e.g., NF4/FP4) using bitsandbytes. Split into multiple cells for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Gemma3ForConditionalGeneration\n",
    "import bitsandbytes as bnb\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load the model\n",
    "# -----------------------------\n",
    "try:\n",
    "    model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        dtype=torch.bfloat16\n",
    "    )\n",
    "    print(f\"Successfully loaded {model_name} with 4-bit NF4 quantization\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model with 4-bit quantization: {e}\")\n",
    "    print(\"Falling back to loading model without quantization...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    print(\"Loaded model without quantization - you can apply custom 4-bit quantization manually.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"Tokenizer for {model_name} loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f12dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_4bit(module, dtype=\"nf4\"):\n",
    "    \"\"\"\n",
    "    Replace Linear weights in the model with 4-bit quantized versions (NF4/FP4) using bitsandbytes.\n",
    "    Non-linear layers are left unchanged. Assumes recent bitsandbytes with Float4Params.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_modules():\n",
    "        if isinstance(child, torch.nn.Linear):\n",
    "            try:\n",
    "                # Quantize weights\n",
    "                q = bnb.nn.LinearNF4(\n",
    "                    child.in_features,\n",
    "                    child.out_features,\n",
    "                    bias=(child.bias is not None)\n",
    "                )\n",
    "                q.weight.data.copy_(child.weight.data)\n",
    "                if child.bias is not None:\n",
    "                    q.bias.data.copy_(child.bias.data)\n",
    "                # Wrap in nn.Parameter if needed\n",
    "                if not isinstance(q, torch.nn.Parameter):\n",
    "                    q = torch.nn.Parameter(q)\n",
    "                # Assign quantized weights back\n",
    "                child.weight = q\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping quantization for {name}: {e}\")\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5252ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = convert_to_4bit(model, dtype=\"nf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d09689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# -----------------------------\n",
    "# Save directory (Linux/WSL style)\n",
    "# -----------------------------\n",
    "directory = \"/mnt/d/Model Folder/modcord_custom_models/gemma-3-4b-it-nf4\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "model_to_save = quantized_model\n",
    "\n",
    "# -----------------------------\n",
    "# Save the model and tokenizer\n",
    "# -----------------------------\n",
    "model_to_save.save_pretrained(directory)\n",
    "tokenizer.save_pretrained(directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {directory}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Print model info\n",
    "# -----------------------------\n",
    "print(\"\\nModel details:\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "# dtype might not exist on the model object if using bitsandbytes\n",
    "dtype = getattr(model_to_save, \"dtype\", None)\n",
    "print(f\"Model dtype: {dtype if dtype is not None else 'mixed/4-bit'}\")\n",
    "\n",
    "if hasattr(model_to_save.config, 'quantization_config') and model_to_save.config.quantization_config:\n",
    "    print(f\"Quantization config: {model_to_save.config.quantization_config}\")\n",
    "else:\n",
    "    print(\"No quantization config found in model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b59d47",
   "metadata": {},
   "source": [
    "### Use Sharding. Completely different approach but hopefully works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1735cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from safetensors.torch import load_file, save_file\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "SRC_DIR = r\"/mnt/d/Model Folder/huggingface_model_cache/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee\"\n",
    "OUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/gpt-oss-20b-nf4\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Copy config and tokenizer files\n",
    "# -----------------------------\n",
    "for fname in [\"config.json\", \"generation_config.json\", \"tokenizer.json\", \n",
    "              \"tokenizer_config.json\", \"special_tokens_map.json\"]:\n",
    "    src_path = os.path.join(SRC_DIR, fname)\n",
    "    dst_path = os.path.join(OUT_DIR, fname)\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "# -----------------------------\n",
    "# Process each shard\n",
    "# -----------------------------\n",
    "for fname in os.listdir(SRC_DIR):\n",
    "    if fname.endswith(\".safetensors\") and \"model\" in fname:\n",
    "        src_shard = os.path.join(SRC_DIR, fname)\n",
    "        out_shard = os.path.join(OUT_DIR, fname.replace(\".safetensors\", \"-bf16.safetensors\"))\n",
    "\n",
    "        print(f\"Processing shard: {fname} → {os.path.basename(out_shard)}\")\n",
    "        # Load shard to GPU\n",
    "        state_dict = load_file(src_shard, device=\"cuda\")\n",
    "        \n",
    "        # Convert FP16 → BF16, leave others intact\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            if v.dtype == torch.float16:\n",
    "                new_state_dict[k] = v.to(torch.bfloat16)\n",
    "            else:\n",
    "                new_state_dict[k] = v\n",
    "        \n",
    "        # Save new shard\n",
    "        save_file(new_state_dict, out_shard)\n",
    "\n",
    "        # Explicitly free memory\n",
    "        del state_dict\n",
    "        del new_state_dict\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  # optional if using GPU\n",
    "\n",
    "print(f\"\\nAll shards converted. BF16 model ready at: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e2dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from safetensors.torch import load_file\n",
    "OUT_DIR = r\"/mnt/d/Model Folder/modcord_custom_models/qwen3-4b-instruct-nf4\"\n",
    "shard_path = os.path.join(OUT_DIR, \"model.safetensors\")\n",
    "\n",
    "state_dict = load_file(shard_path, device=\"cuda\")\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    print(k, v.shape, v.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file, save_file\n",
    "import os\n",
    "\n",
    "model_dir = \"/mnt/d/Model Folder/modcord_custom_models/qwen3-4b-instruct-nf4\"\n",
    "shard_files = sorted([f for f in os.listdir(model_dir) if f.endswith(\".safetensors\")])\n",
    "\n",
    "combined = {}\n",
    "for shard_file in shard_files:\n",
    "    shard_path = os.path.join(model_dir, shard_file)\n",
    "    shard_data = load_file(shard_path)\n",
    "    combined.update(shard_data)\n",
    "\n",
    "# Save as one file\n",
    "save_file(combined, os.path.join(model_dir, \"model.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75204baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry-new-account/modcord/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/henry-new-account/modcord/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/home/henry-new-account/modcord/venv/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 709.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processor and tokenizer (use_fast=True) to: /mnt/d/Model Folder/modcord_custom_models/gemma-3-4b-it-nf4\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "TARGET_DIR = Path(r\"/mnt/d/Model Folder/modcord_custom_models/gemma-3-4b-it-nf4\")\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processor (multimodal)\n",
    "proc = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\", trust_remote_code=True, use_fast=True)\n",
    "proc.save_pretrained(TARGET_DIR)\n",
    "\n",
    "# Tokenizer (ensure fast tokenizer saved if available)\n",
    "tok = AutoTokenizer.from_pretrained(\"google/gemma-3-4b-it\", trust_remote_code=True, use_fast=True)\n",
    "tok.save_pretrained(TARGET_DIR)\n",
    "\n",
    "print(\"Saved processor and tokenizer (use_fast=True) to:\", TARGET_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
