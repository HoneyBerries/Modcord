{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947ae66719359057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry-new-account/projects/modcord/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a2aa94ba6ea69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:52<00:00, 17.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "Tokenizer loaded successfully\n",
      "Processor loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_type=torch.bfloat16\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cuda\",\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True\n",
    "    ).eval()\n",
    "\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    print(\"Tokenizer loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}\")\n",
    "    tokenizer = None\n",
    "\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    print(\"Processor loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor: {e}\")\n",
    "    processor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee358505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): Qwen3ForCausalLM(\n",
       "    (model): Qwen3Model(\n",
       "      (embed_tokens): Embedding(151936, 2560)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen3DecoderLayer(\n",
       "          (self_attn): Qwen3Attention(\n",
       "            (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "            (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "            (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "            (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Qwen3MLP(\n",
       "            (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): Qwen3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915c08008e6560b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant that provides detailed and informative answers to user queries.\n",
    "When responding, ensure that your answers are clear and easy to understand.'\"\"\"\n",
    "\n",
    "def chat_with_ai(model, tokenizer, processor):\n",
    "    # Initialize conversation history\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}\n",
    "    ]\n",
    "\n",
    "    print(\"Chat initialized. Type 'exit' to end the conversation.\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_message = input(\"You: \")\n",
    "\n",
    "        if user_message.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Add user message to conversation history\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        # Apply chat template with proper tokenization\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move inputs to device and create attention mask\n",
    "        input_ids = inputs.to(\"cuda\")\n",
    "        attention_mask = torch.ones_like(input_ids).to(\"cuda\")\n",
    "        prompt_length = input_ids.shape[1]\n",
    "\n",
    "        # Generate response and measure time\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=8192,\n",
    "                temperature=0.7,\n",
    "                top_p=0.7,\n",
    "                repetition_penalty=1.2,\n",
    "                do_sample=True,\n",
    "                use_cache=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Extract just the new tokens\n",
    "        new_tokens = output[0][prompt_length:]\n",
    "        assistant_response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Print the response\n",
    "        print(f\"AI: {assistant_response}\")\n",
    "\n",
    "        # Calculate and print tokens per second\n",
    "        num_tokens = new_tokens.shape[0] if hasattr(new_tokens, 'shape') else len(new_tokens)\n",
    "        elapsed = end_time - start_time\n",
    "        tps = num_tokens / elapsed if elapsed > 0 else 0\n",
    "        print(f\"Tokens generated: {num_tokens} | Time: {elapsed:.2f}s | Tokens/sec: {tps:.2f}\")\n",
    "\n",
    "        # Add assistant response to conversation history\n",
    "        conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c21ff4f595c411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat initialized. Type 'exit' to end the conversation.\n",
      "AI: We want to compute the integral:\n",
      "\n",
      "$$\n",
      "\\int \\frac{\\arctan(x)}{x^2} \\, dx\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Use Integration by Parts\n",
      "\n",
      "Let’s use **integration by parts**, which is useful when we have a product of functions — one of them being an inverse trig function like $\\arctan x$, and another simple function.\n",
      "\n",
      "Recall:\n",
      "$$\n",
      "\\int u \\, dv = uv - \\int v \\, du\n",
      "$$\n",
      "\n",
      "Set:\n",
      "- $u = \\arctan(x)$ → then $du = \\dfrac{1}{1+x^2}\\,dx$\n",
      "- $dv = \\dfrac{1}{x^2} \\, dx$ → so $v = \\int x^{-2} \\, dx = -\\dfrac{1}{x}$\n",
      "\n",
      "Now apply integration by parts:\n",
      "\n",
      "$$\n",
      "\\int \\frac{\\arctan(x)}{x^2} \\, dx = -\\frac{\\arctan(x)}{x} + \\int \\frac{1}{x(1+x^2)} \\, dx\n",
      "$$\n",
      "\n",
      "So now our problem reduces to evaluating:\n",
      "$$\n",
      "\\int \\frac{1}{x(1+x^2)} \\, dx\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Partial Fraction Decomposition\n",
      "\n",
      "Decompose:\n",
      "$$\n",
      "\\frac{1}{x(1+x^2)} = \\frac{A}{x} + \\frac{Bx + C}{1+x^2}\n",
      "$$\n",
      "\n",
      "Multiply both sides by $x(1+x^2)$:\n",
      "\n",
      "$$\n",
      "1 = A(1+x^2) + (Bx+C)(x)\n",
      "$$\n",
      "\n",
      "Expand right-hand side:\n",
      "$$\n",
      "1 = A + Ax^2 + Bx^2 + Cx = A + (A+B)x^2 + Cx\n",
      "$$\n",
      "\n",
      "Match coefficients:\n",
      "\n",
      "- Constant term: $A = 1$\n",
      "- Coefficient of $x$: $C = 0$\n",
      "- Coefficient of $x^2$: $A + B = 0$\n",
      "\n",
      "Since $A=1$, then $1 + B = 0$ ⇒ $B = -1$\n",
      "\n",
      "Thus,\n",
      "$$\n",
      "\\frac{1}{x(1+x^2)} = \\frac{1}{x} - \\frac{x}{1+x^2}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Integrate Term-by-Term\n",
      "\n",
      "$$\n",
      "\\int \\left(\\frac{1}{x} - \\frac{x}{1+x^2}\\right)\\, dx = \\int \\frac{1}{x} \\, dx - \\int \\frac{x}{1+x^2} \\, dx\n",
      "$$\n",
      "\n",
      "First integral:\n",
      "$$\n",
      "\\int \\frac{1}{x} \\, dx = \\ln|x|\n",
      "$$\n",
      "\n",
      "Second integral: Let $u = 1+x^2$, $du = 2x\\,dx$. Then:\n",
      "$$\n",
      "\\int \\frac{x}{1+x^2} \\, dx = \\frac{1}{2} \\int \\frac{2x}{1+x^2} \\, dx = \\frac{1}{2} \\ln|1+x^2|\n",
      "$$\n",
      "\n",
      "So overall:\n",
      "$$\n",
      "\\int \\frac{1}{x(1+x^2)} \\, dx = \\ln|x| - \\frac{1}{2} \\ln(1+x^2) + C\n",
      "$$\n",
      "\n",
      "(Since $1+x^2 > 0$, absolute value can be dropped.)\n",
      "\n",
      "This simplifies as:\n",
      "$$\n",
      "\\ln|x| - \\frac{1}{2} \\ln(1+x^2) = \\ln\\left|\\frac{x}{\\sqrt{1+x^2}}\\right|\n",
      "$$\n",
      "\n",
      "But for simplicity in final answer, keep it in log form.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Answer:\n",
      "\n",
      "Putting everything together from earlier:\n",
      "\n",
      "$$\n",
      "\\int \\frac{\\arctan(x)}{x^2} \\, dx = -\\frac{\\arctan(x)}{x} + \\left( \\ln|x| - \\frac{1}{2} \\ln(1+x^2) \\right) + C\n",
      "$$\n",
      "\n",
      "Or more neatly written:\n",
      "\n",
      "$$\n",
      "\\boxed{\n",
      "\\int \\frac{\\arctan(x)}{x^2} \\, dx = -\\frac{\\arctan(x)}{x} + \\ln\\left|\\frac{x}{\\sqrt{1+x^2}}\\right| + C\n",
      "}\n",
      "$$\n",
      "\n",
      "Alternatively, using logarithmic identities:\n",
      "$$\n",
      "\\ln\\left|\\frac{x}{\\sqrt{1+x^2}}\\right| = \\ln|x| - \\frac{1}{2} \\ln(1+x^2)\n",
      "$$\n",
      "\n",
      "Both forms are correct. The first boxed version is compact and elegant.\n",
      "\n",
      "✅ So the final result is:\n",
      "\n",
      "$$\n",
      "\\boxed{-\\frac{\\arctan(x)}{x} + \\ln\\left|\\frac{x}{\\sqrt{1+x^2}}\\right| + C}\n",
      "$$\n",
      "\n",
      "(Note: For real-valued integrals with $x>0$, you may drop absolutes if context allows.)\n",
      "Tokens generated: 1077 | Time: 118.41s | Tokens/sec: 9.10\n"
     ]
    }
   ],
   "source": [
    "if model is None or tokenizer is None:\n",
    "    print(\"Error: Model or tokenizer failed to load. Please check the error messages above and verify:\")\n",
    "    print(\"1. The model path exists and is correct\")\n",
    "    print(f\"2. You have sufficient GPU memory (this model requires ~{15} GB VRAM)\")\n",
    "    print(\"3. The model files are not corrupted\")\n",
    "else:\n",
    "    chat_with_ai(model, tokenizer, processor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
